How to Configure the Kafka Consumer & Producer to work with Amazon MSK
----------------------------------------------------------------------

References
----------
https://docs.aws.amazon.com/msk/latest/developerguide/create-cluster.html
https://docs.aws.amazon.com/msk/latest/developerguide/msk-encryption.html#msk-encryption-in-transit
https://medium.com/egen/securing-kafka-cluster-using-sasl-acl-and-ssl-dec15b439f9d
https://aws.amazon.com/blogs/big-data/securing-apache-kafka-is-easy-and-familiar-with-iam-access-control-for-amazon-msk/
https://docs.aws.amazon.com/msk/latest/developerguide/public-access.html
https://docs.aws.amazon.com/msk/latest/developerguide/public-access.html



Part 1:  Setup the Amazon MSK Cluster
-------------------------------------
 1. login to aws.com

 2. Create a MSK cluster:  demo-cluster-1
    a. Go to MSK
    b. On the left, choose Clusters
    c. Press "Create cluster"
    d. In the Create cluster page

        Creation Method:        Quick create
        Cluster Name:           demo-cluster-1
        Cluster Type:           Provisioned

        Apache Kafka version:   2.8.1

        Broker Type:            kafka.t3.small

        EBS Storage per broker:  1 GiB

        Press "Create cluster"


        W A I T    U P    T O      1 5      M I N     (but keep following procedures while the cluster is starting)


 3. Create an IAM Policy:  access-to-demo-1-msk-cluster-policy
    This policy grants access to create topics on the cluster and to send data to those topics
    a. Go to IAM
    b. In the nav pane, choose Policies.
    c. Press "Create Policy"
    d. Choose the JSON tab
    e. Copy this to the Policy editor

            {
                "Version": "2012-10-17",
                "Statement": [
                    {
                        "Effect": "Allow",
                        "Action": [
                            "kafka-cluster:Connect",
                            "kafka-cluster:AlterCluster",
                            "kafka-cluster:DescribeCluster"
                        ],
                        "Resource": [
                            "arn:aws:kafka:MY_REGION:MY_ACCOUNT_ID:cluster/MY_MSK_CLUSTER_NAME/*"
                        ]
                    },
                    {
                        "Effect": "Allow",
                        "Action": [
                            "kafka-cluster:*Topic*",
                            "kafka-cluster:WriteData",
                            "kafka-cluster:ReadData"
                        ],
                        "Resource": [
                            "arn:aws:kafka:MY_REGION:MY_ACCOUNT_ID:topic/MY_MSK_CLUSTER_NAME/*"
                        ]
                    },
                    {
                        "Effect": "Allow",
                        "Action": [
                            "kafka-cluster:AlterGroup",
                            "kafka-cluster:DescribeGroup"
                        ],
                        "Resource": [
                            "arn:aws:kafka:MY_REGION:MY_ACCOUNT_ID:group/MY_MSK_CLUSTER_NAME/*"
                        ]
                    }
                ]
            }


    f. Update this text with your info
       1) Replace MY_MSK_CLUSTER_NAME  --> Your MSK cluster name  -- e.g., demo-cluster-3
       2) Replace MY_ACCOUNT_ID        --> Your account id        -- e.g., 524647912468    Click on your name in the upper right corner and you will see it
       3) Replace MY_REGION            --> With account region    -- e.g., us-east-1       Look at the url of your console and you should see it

       When finished, my policy looked like this:

        {
            "Version": "2012-10-17",
            "Statement": [
                {
                    "Effect": "Allow",
                    "Action": [
                        "kafka-cluster:Connect",
                        "kafka-cluster:AlterCluster",
                        "kafka-cluster:DescribeCluster"
                    ],
                    "Resource": [
                        "arn:aws:kafka:us-east-1:524647912468:cluster/demo-cluster-1/*"
                    ]
                },
                {
                    "Effect": "Allow",
                    "Action": [
                        "kafka-cluster:*Topic*",
                        "kafka-cluster:WriteData",
                        "kafka-cluster:ReadData"
                    ],
                    "Resource": [
                        "arn:aws:kafka:us-east-1:524647912468:topic/demo-cluster-1/*"
                    ]
                },
                {
                    "Effect": "Allow",
                    "Action": [
                        "kafka-cluster:AlterGroup",
                        "kafka-cluster:DescribeGroup"
                    ],
                    "Resource": [
                        "arn:aws:kafka:us-east-1:524647912468:group/demo-cluster-1/*"
                    ]
                }
            ]
        }



     g. Press "Next: Tags"
     h. In the "Review and Create" page
        Name:  access-to-demo-1-msk-cluster-policy
        Press "Create policy"


  4. Create an IAM role:   access-to-demo-1-msk-cluster-role
     NOTE:  This role has the above policy attached to it
     a. On the nav pane, choose Roles.
     b. Press "Create role"
     c. Under Common use cases, choose EC2,
        Press "Next"

     d. In "Add permissions"
        Enter the name of the policy in the search box -- e.g., access-to-demo-1-msk-cluster-policy
        Check "access-to-demo-1-msk-cluster-policy"
        Press "Next"

     e. In Name, review, and create
        Role Name:  access-to-demo-1-msk-cluster-role
        Press "Create role"


 5. Create an instance:  msk-instance
    NOTE:  We will use this instance to run a topic and verify that instances have access to the Kafka topic
    a. Go to EC2
    b. Press "Launch instance" -> Launch instance
    c. In the Launch an instance page
       Name:           msk-instance
       Application:    Amazon Linux 2 AMI (HVM) - Kernel 5.10, SSD Volume Type
       Architecture:   64-bit
       Instance Type:  t2.micro

    d. In Key pair (login)
       Press "Create a new key pair"
         Key pair name:        msk-key-pair
         Key pair type:        RSA
         Private key format:   .pem
         Press "Create key pair"

         You will be prompted to save msk-key-pair.pem
         -- Save it to your ~/Downloads/msk-key-pair.pem

    e. In Network settings, press "Edit"
       VPC:                         (default)
       Auto-assign public IP:       Enable
       Firewall (Security groups):  Create security group
       Security Group name:         security-group-msk-instance-1

       NOTE:  By default ssh should be allowed from anywhere


    f. In Advanced details
       IAM instance profile:  access-to-demo-1-msk-cluster-role     (This is the role you created earlier)

    g. Press "Launch instance"

        W A I T      5      M I N U T E S      F O R     EC2 Instance      T O     B E     R U N N I N G

        W A I T    F O R     Amazon MSK CLUSTER     T O    B E    A C T I V E


    h. Adjust the MSK's Security Group so it allows inbound traffic
       NOTE:  This is needed so that the running instance can connect to the MSK Cluster (and create a topic)
       a. Go to Amazon MSK
       b. Click on your cluster
       c. Click on the "Properties" tab
       d. Scroll down to Network Settings
       e. Click the link to the security group
       f. Press "Edit inbound rules"
       g. Press "Add Rule"   [DON'T MISS THIS STEP]
         Type:  All Traffic
         Source:  Custom
                  Search for your security-group-msk-instance-1

         NOW, THERE SHOULD BE 2 RULES for this security group

         Press "Save rules"


 6. Create a kafka topic  (from the running instance)
    NOTE:  We have granted access to the running instance access to the MSK cluster
           So processes (running on the instance) can push/consume from kafka topics

    a. Get the version of kafka from MSK
       1) Go to MSK
       2) Look for your clusters
            demo-cluster-1
            -- Get the Apache Kakfa version -- e.g., 2.8.1

    b. Get the SSH credentials to connect to your running instance
       1) Go to EC2
       2) Select Instance
       3) Check your msk-instance
       4) Pull Actions -> Connect
       5) Select the "SSH client" tab
          -- You should see the ssh command to connect
                 ssh -i "msk-key-pair.pem" ec2-user@ec2-3-83-99-235.compute-1.amazonaws.com

    c. Copy the msk-key-pair.pem to your ~/.ssh directory
       unix> mv ~/Downloads/msk-key-pair.pem ~/.ssh
       unix> chmod go-rwx ~/.ssh/msk-key-pair.pem      # prevent other users from reading/writing this file

    d. SSH to your box
       unix> ssh -i ~/.ssh/msk-key-pair.pem ec2-user@ec2-3-83-99-235.compute-1.amazonaws.com


    e. Install Java-11 on the instance
       aws-instance-unix> sudo yum install java-11

    f. Install Telnet on the instance
       aws-instance-unix> sudo yum install telnet

    g. Download kafka 2.8.1 to this instance
       aws-instance-unix> cd
       aws-instance-unix> wget https://archive.apache.org/dist/kafka/2.8.1/kafka_2.12-2.8.1.tgz
       aws-instance-unix> tar -xzf kafka_2.12-2.8.1.tgz
       aws-instance-unix> rm kafka_2.12-2.8.1.tgz

    h. Download the msk iam jar to the kafka_2.12-2.8.1/libs directory
       aws-instance-unix> cd ~/kafka_2.12-2.8.1/libs
       aws-instance-unix> wget https://github.com/aws/aws-msk-iam-auth/releases/download/v1.1.6/aws-msk-iam-auth-1.1.6-all.jar

    i. Create client.properties in the kafka_2.12-2.8.1/bin directory
       aws-instance-unix> cd ~/kafka_2.12-2.8.1/bin
       aws-instance-unix> vi client.properties

            security.protocol=SASL_SSL
            sasl.mechanism=AWS_MSK_IAM
            sasl.jaas.config=software.amazon.msk.auth.iam.IAMLoginModule required;
            sasl.client.callback.handler.class=software.amazon.msk.auth.iam.IAMClientCallbackHandler

    i. Get the bootstrap server hostnames
       1) Go to the MSK console
       2) Click on your demo-cluster-1
       3) Press "View client information"
       4) Under Bootstrap servers
          Copy the TLS endpoint for private endpoint -- e.g., b-3.democluster1.gh2mtx.c11.kafka.us-east-1.amazonaws.com:9098,b-1.democluster1.gh2mtx.c11.kafka.us-east-1.amazonaws.com:9098,b-2.democluster1.gh2mtx.c11.kafka.us-east-1.amazonaws.com:9098

    j. Verify that you can reach one of the boot strap servers
       unix> telnet b-3.democluster1.gh2mtx.c11.kafka.us-east-1.amazonaws.com 9098
       -- Verify that it says connected to....
       -- If it times-out, then STOP HERE and adjust your security groups

    k. Create a topic called:  MSKTutorialTopic
       aws-instance-unix> cd ~/kafka_2.12-2.8.1/bin
       aws-instance-unix> export TOPIC_NAME=MSKTutorialTopic
       aws-instance-unix> export BOOTSTRAP_SERVER=b-3.democluster1.gh2mtx.c11.kafka.us-east-1.amazonaws.com:9098    # We only need 1 to create the topic
       aws-instance-unix> ./kafka-topics.sh --create --bootstrap-server $BOOTSTRAP_SERVER --command-config client.properties --replication-factor 3 --partitions 1 --topic ${TOPIC_NAME}



Part 2:  Configure springBootKafka to use SASL_SSL and connect to our public Amazon MSK Cluster
-----------------------------------------------------------------------------------------------
  1. Add the maven dependency to the producer and consumer pom.xml files:
     a. Add this dependency to consumer/pom.xml
        <dependency>
            <groupId>software.amazon.msk</groupId>
            <artifactId>aws-msk-iam-auth</artifactId>
            <version>1.1.6</version>
        </dependency>

     b. Add this dependency to producer/pom.xml
        <dependency>
            <groupId>software.amazon.msk</groupId>
            <artifactId>aws-msk-iam-auth</artifactId>
            <version>1.1.6</version>
        </dependency>



  2. Tell the Producer Configuration to have some additional properties:
     a. Edit MyProducerConfig

     b. Add the properties to the kafka property configuration
             // AWS Kafka properties
             // NOTE:  You need the semi-colon at the end of the "sasl.jaas.config" configuration
             props.put("ssl.truststore.location",            this.trustStoreFilePath);
             props.put("security.protocol",                  "SASL_SSL");
             props.put("sasl.mechanism",                     "AWS_MSK_IAM");
             props.put("sasl.jaas.config",                   "software.amazon.msk.auth.iam.IAMLoginModule required;");
             props.put("sasl.client.callback.handler.class", "software.amazon.msk.auth.iam.IAMClientCallbackHandler");


        When finished, the MyProducerConfig should look something like this

            package com.whatever.producer;

            import org.apache.kafka.clients.producer.ProducerConfig;
            import org.apache.kafka.common.serialization.StringSerializer;
            import org.springframework.beans.factory.annotation.Value;
            import org.springframework.context.annotation.Bean;
            import org.springframework.context.annotation.Configuration;
            import org.springframework.kafka.core.DefaultKafkaProducerFactory;
            import org.springframework.kafka.core.KafkaTemplate;
            import org.springframework.kafka.core.ProducerFactory;

            import java.util.HashMap;
            import java.util.Map;

            /**
             * MyProducerConfig
             */
            @Configuration
            public class MyProducerConfig {
                @Value("${kafka.bootstrap-servers}")
                private String bootstrapServers;

                @Value("${kafka.truststore-filepath}")                // Inject the truststore file path
                private String trustStoreFilePath;

                @Bean
                public Map<String, Object> producerConfigs() {
                    Map<String, Object> props = new HashMap<>();

                    // list of host:port pairs used for establishing the initial connections to the Kakfa cluster
                    props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers);
                    props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class);
                    props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class);

                    // AWS Kafka properties
                    // NOTE:  You need the semi-colon at the end of the "sasl.jaas.config" configuration
                    props.put("security.protocol",                  "SASL_SSL");
                    props.put("sasl.mechanism",                     "AWS_MSK_IAM");
                    props.put("sasl.jaas.config",                   "software.amazon.msk.auth.iam.IAMLoginModule required;");
                    props.put("sasl.client.callback.handler.class", "software.amazon.msk.auth.iam.IAMClientCallbackHandler");

                    return props;
                }

                @Bean
                public ProducerFactory<String, String> producerFactory() {
                    Map<String, Object> configs = producerConfigs();
                    return new DefaultKafkaProducerFactory<>(configs);
                }

                @Bean
                public KafkaTemplate<String, String> kafkaTemplate() {
                    return new KafkaTemplate<>(producerFactory());
                }

            }



  3. Adjust the Consumer Kafka Configuration to use AWS MSK's security
     a. Edit MyConsumerConfig


     b. Add the properties to the kafka property configuration     a. Inject the truststore file path
             // AWS Kafka properties
             // NOTE:  You need the semi-colon at the end of the "sasl.jaas.config" configuration
             props.put("ssl.truststore.location",            this.trustStoreFilePath);
             props.put("security.protocol",                  "SASL_SSL");
             props.put("sasl.mechanism",                     "AWS_MSK_IAM");
             props.put("sasl.jaas.config",                   "software.amazon.msk.auth.iam.IAMLoginModule required;");
             props.put("sasl.client.callback.handler.class", "software.amazon.msk.auth.iam.IAMClientCallbackHandler");


        When finished, the MyProducerConfig should look something like this

            package com.whatever.consumer;

            import org.apache.kafka.clients.consumer.ConsumerConfig;
            import org.apache.kafka.common.serialization.StringDeserializer;
            import org.springframework.beans.factory.annotation.Value;
            import org.springframework.context.annotation.Bean;
            import org.springframework.context.annotation.Configuration;
            import org.springframework.kafka.annotation.EnableKafka;
            import org.springframework.kafka.config.ConcurrentKafkaListenerContainerFactory;
            import org.springframework.kafka.config.KafkaListenerContainerFactory;
            import org.springframework.kafka.core.*;
            import org.springframework.kafka.listener.ConcurrentMessageListenerContainer;

            import java.util.HashMap;
            import java.util.Map;

            /**
             * MyConsumerConfig
             */
            @EnableKafka
            @Configuration
            public class MyConsumerConfig {
                @Value("${kafka.bootstrap-servers}")
                private String bootstrapServers;

                @Value("${kafka.truststore-filepath}")
                private String trustStoreFilePath;

                @Bean
                public Map<String, Object> consumerConfig() {
                    Map<String, Object> props = new HashMap<>();

                    // list of host:port pairs used for establishing the initial connections to the Kakfa cluster
                    props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG,  bootstrapServers);
                    props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);
                    props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);

                    // allows a pool of processes to divide the work of consuming records
                    props.put(ConsumerConfig.GROUP_ID_CONFIG, "updates");

                    // automatically reset the offset to the earliest offset
                    props.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, "earliest");

                    // Pull at most 10 records at a time
                    props.put(ConsumerConfig.MAX_POLL_RECORDS_CONFIG, 10);


                    // AWS Kafka properties
                    // NOTE:  You need the semi-colon at the end of the "sasl.jaas.config" configuration
                    props.put("security.protocol",                  "SASL_SSL");
                    props.put("sasl.mechanism",                     "AWS_MSK_IAM");
                    props.put("sasl.jaas.config",                   "software.amazon.msk.auth.iam.IAMLoginModule required;");
                    props.put("sasl.client.callback.handler.class", "software.amazon.msk.auth.iam.IAMClientCallbackHandler");


                    return props;
                }

                @Bean
                public ConsumerFactory<String, String> consumerFactory() {
                    return new DefaultKafkaConsumerFactory<>(consumerConfig());
                }

                @Bean
                public KafkaListenerContainerFactory<ConcurrentMessageListenerContainer<String, String>> kafkaListenerContainerFactory() {
                    ConcurrentKafkaListenerContainerFactory<String, String> factory =  new ConcurrentKafkaListenerContainerFactory<>();
                    factory.setConsumerFactory(consumerFactory());
                    return factory;
                }

            }




 4. Build the JAR and upload it to the running instance
    a. Compile the code into an uber jar
       unix> mvn clean package

    b. Upload the jars
       NOTE:  THe destination hostname can be found in Running Instances -> Select Instance -> Public IPv4 DNS
       unix> scp -i ~/.ssh/msk-key-pair.pem  ./producer/target/producer-1.0-SNAPSHOT.jar  ec2-user@ec2-3-83-99-235.compute-1.amazonaws.com:/tmp
       unix> scp -i ~/.ssh/msk-key-pair.pem  ./consumer/target/consumer-1.0-SNAPSHOT.jar  ec2-user@ec2-3-83-99-235.compute-1.amazonaws.com:/tmp



 5. Try to run the jars on the running instance
    a. ssh to the running instance
       unix> ssh -i "msk-key-pair.pem" ec2-user@ec2-3-83-99-235.compute-1.amazonaws.com

    b. Move the jars from /tmp to /home/ec2-user
       aws-instance-unix> cd
       aws-instance-unix> cp /tmp/consumer-1.0-SNAPSHOT.jar .
       aws-instance-unix> cp /tmp/producer-1.0-SNAPSHOT.jar .

    c. Get the bootstrap server hostnames
       1) Go to the MSK console
       2) Click on your demo-cluster-1
       3) Press "View client information"
       4) Under Bootstrap servers
          Copy the TLS endpoint for private endpoint -- e.g., b-3.democluster1.gh2mtx.c11.kafka.us-east-1.amazonaws.com:9098,b-1.democluster1.gh2mtx.c11.kafka.us-east-1.amazonaws.com:9098,b-2.democluster1.gh2mtx.c11.kafka.us-east-1.amazonaws.com:9098

    d. Run the producer (to push messages)
       aws-instance-unix> export TOPIC_NAME=MSKTutorialTopic
       aws-instance-unix> export BOOTSTRAP_SERVERS=b-3.democluster1.gh2mtx.c11.kafka.us-east-1.amazonaws.com:9098,b-1.democluster1.gh2mtx.c11.kafka.us-east-1.amazonaws.com:9098,b-2.democluster1.gh2mtx.c11.kafka.us-east-1.amazonaws.com:9098
       aws-instance-unix> java -Dkafka.bootstrap-servers=${BOOTSTRAP_SERVERS}  -Dkafka.topic-name=${TOPIC_NAME} -jar ./producer-1.0-SNAPSHOT.jar


    e. Run the consumer (to consume messages)
       1) ssh to the running instance in a *SECOND* window
          unix> ssh -i "msk-key-pair.pem" ec2-user@ec2-3-83-99-235.compute-1.amazonaws.com

       2) Start the consumer
          aws-instance-unix> export TOPIC_NAME=MSKTutorialTopic
          aws-instance-unix> export BOOTSTRAP_SERVERS=b-3.democluster1.gh2mtx.c11.kafka.us-east-1.amazonaws.com:9098,b-1.democluster1.gh2mtx.c11.kafka.us-east-1.amazonaws.com:9098,b-2.democluster1.gh2mtx.c11.kafka.us-east-1.amazonaws.com:9098
          aws-instance-unix> java -Dkafka.bootstrap-servers=${BOOTSTRAP_SERVERS}  -Dkafka.topic-name=${TOPIC_NAME} -jar ./consumer-1.0-SNAPSHOT.jar

